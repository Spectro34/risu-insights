---
# Deployment playbook for Qwen 30B model with ansible-runner MCP server
# This playbook shows how to deploy a GPU-enabled Ollama host, pull the
# Qwen 30B model, and configure mcphost with the Ansible Runner MCP server.

- name: Deploy Qwen 30B with Ansible Runner MCP
  hosts: localhost
  become: yes
  vars:
    ollama_model: "qwen2.5:32b"
    ollama_gpu_enabled: true
    ollama_pull_models:
      - "qwen2.5:32b"

    mcphost_temperature: 0.2
    mcphost_max_tokens: 4096
    mcphost_max_steps: 30
    risu_mcp_configure_endpoint: false

    mcphost_local_servers:
      - name: "ansible-runner"
        command: ["python3", "{{ playbook_dir }}/ansible-runner-mcp/ansible_runner_mcp_fastmcp.py"]
        cwd: "{{ playbook_dir }}"
        description: "Ansible Runner MCP server - AWX-style consolidated tools"
        env:
          ANSIBLE_RUNNER_PRIVATE_DATA_DIR: "{{ ansible_env.HOME }}/github/runner/ansible-runner"
          PYTHONUNBUFFERED: "1"
  
  tasks:
    - name: Display deployment information
      debug:
        msg: |
          ╔══════════════════════════════════════════════════════════╗
          ║  Qwen 30B + Ansible Runner MCP Deployment                ║
          ╠══════════════════════════════════════════════════════════╣
          ║  Model: {{ ollama_model }}
          ║  GPU: {{ 'Enabled' if ollama_gpu_enabled else 'Disabled' }}
          ║  MCP Server: ansible-runner
          ╚══════════════════════════════════════════════════════════╝
    
    - name: Ensure ansible-runner MCP server directory exists
      file:
        path: "{{ playbook_dir }}/ansible-runner-mcp"
        state: directory
        mode: '0755'
    
    - name: Install fastmcp Python package
      pip:
        name: fastmcp
        state: present
      become: yes
    
    - name: Install pyyaml Python package
      pip:
        name: pyyaml
        state: present
      become: yes

    - name: Verify ansible-runner MCP server exists
      stat:
        path: "{{ playbook_dir }}/ansible-runner-mcp/ansible_runner_mcp_fastmcp.py"
      register: mcp_server_file
    
    - name: Fail if MCP server not found
      fail:
        msg: "ansible-runner MCP server not found at {{ playbook_dir }}/ansible-runner-mcp/ansible_runner_mcp_fastmcp.py. Ensure you're running from the correct directory."
      when: not mcp_server_file.stat.exists
    
    # Run the ollama_mcphost role
    # Note: Variables are passed from playbook vars automatically
    - name: Deploy Ollama, Qwen model, and mcphost with ansible-runner MCP
      include_role:
        name: ansible-ollama_mcphost
    
    - name: Verify GPU is available for Ollama
      command: nvidia-smi --query-gpu=name --format=csv,noheader
      register: gpu_check
      changed_when: false
      failed_when: false
      when: ollama_gpu_enabled
    
    - name: Display GPU information
      debug:
        msg: |
          ✓ GPU Information:
          {% for gpu in gpu_check.stdout_lines | default([]) %}
          - {{ gpu }}
          {% endfor %}
      when: 
        - ollama_gpu_enabled
        - gpu_check.rc == 0
    
    - name: Test Ollama with GPU
      uri:
        url: "http://localhost:11434/api/generate"
        method: POST
        body_format: json
        body:
          model: "{{ ollama_model }}"
          prompt: "Test GPU inference"
          stream: false
        status_code: [200, 400]  # 400 might occur if model not fully loaded
      register: gpu_test
      failed_when: false
      when: ollama_gpu_enabled
    
    - name: Final deployment summary
      debug:
        msg: |
          
          ╔══════════════════════════════════════════════════════════╗
          ║  Deployment Complete!                                    ║
          ╠══════════════════════════════════════════════════════════╣
          ║  Model: {{ ollama_model }}
          ║  GPU: {{ 'Enabled ✓' if ollama_gpu_enabled else 'Disabled' }}
          ║  MCP Server: ansible-runner (configured)
          ║  Config: ~/.mcphost.yml
          ║  
          ║  Start mcphost:
          ║    mcphost
          ║  
          ║  Available MCP Tools (AWX-style consolidated):
          ║    - run_job
          ║    - get_job_results
          ║    - manage_inventory
          ║    - job_history
          ║    - manage_job_template
          ║    - ansible_runner_list_playbooks
          ╚══════════════════════════════════════════════════════════╝
